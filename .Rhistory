#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#' data(KeeneSOC)
#' oss.goofCaret(KeeneSOC)
#'
#'
oss.goofCaret<- function(ModelName){
r<- ModelName
if(r$method=="cubist"){x<- r$pred %>% filter(committees==r$bestTune$committees, neighbors==r$bestTune$neighbors)
}else if(r$method=="rf"|r$method=="qrf"){x<- r$pred %>% filter(mtry==r$bestTune$mtry)
}else if(r$method=="svmRadial"){x<- r$pred %>% filter(sigma==r$bestTune$sigma, C==r$bestTune$C)
}else if(r$method=="lm"|r$method=="lmStepAIC"){x<- r$pred
}else if(r$method=="gbm"){x<- r$pred %>% filter(n.trees==r$bestTune$n.trees, interaction.depth==r$bestTune$interaction.depth, shrinkage==r$bestTune$shrinkage, n.minobsinnode==r$bestTune$n.minobsinnode)
}else if (r$method=="knn"){x<- r$pred %>% filter(k==r$bestTune$k)
}else {print("Not programmed for model")}
y <- x %>%
dplyr::group_by(Resample)%>%
dplyr::do(as.data.frame(oss.goof(predicted=.$pred,observed=.$obs)))
z<- as.data.frame(t(colMeans(y[c(2:7)])))
w<- as.data.frame(t(sapply(y[c(2:7)],stats::sd,na.rm=TRUE)))
z<-cbind(z,w)
rownames(z)<- r$method
colnames(z)[7:12]<- paste0("sd_",names(z[,1:6]))
rm(r,w,x,y)
invisible(as.data.frame(z))
}
oss.goofCaret(KeeneSOC)
d<- oss.goofCaret(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#' data(KeeneSOC)
#' oss.goofCaret(KeeneSOC)
#'
#'
oss.goofCaret<- function(ModelName){
r<- ModelName
if(r$method=="cubist"){x<- r$pred %>% filter(committees==r$bestTune$committees, neighbors==r$bestTune$neighbors)
}else if(r$method=="rf"|r$method=="qrf"){x<- r$pred %>% filter(mtry==r$bestTune$mtry)
}else if(r$method=="svmRadial"){x<- r$pred %>% filter(sigma==r$bestTune$sigma, C==r$bestTune$C)
}else if(r$method=="lm"|r$method=="lmStepAIC"){x<- r$pred
}else if(r$method=="gbm"){x<- r$pred %>% filter(n.trees==r$bestTune$n.trees, interaction.depth==r$bestTune$interaction.depth, shrinkage==r$bestTune$shrinkage, n.minobsinnode==r$bestTune$n.minobsinnode)
}else if (r$method=="knn"){x<- r$pred %>% filter(k==r$bestTune$k)
}else {print("Not programmed for model")}
y <- x %>%
dplyr::group_by(Resample)%>%
dplyr::do(as.data.frame(oss.goof(predicted=.$pred,observed=.$obs)))
z<- as.data.frame(t(colMeans(y[c(2:7)])))
w<- as.data.frame(t(sapply(y[c(2:7)],stats::sd,na.rm=TRUE)))
z<-cbind(z,w)
rownames(z)<- r$method
colnames(z)[7:12]<- paste0("sd_",names(z[,1:6]))
rm(r,w,x,y)
as.data.frame(z)
}
d<- oss.goofCaret(KeeneSOC)
oss.goofCaret(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#' data(KeeneSOC)
#' oss.goofCaret(KeeneSOC)
#'
#'
oss.goofCaret<- function(ModelName){
r<- ModelName
if(r$method=="cubist"){x<- r$pred %>% filter(committees==r$bestTune$committees, neighbors==r$bestTune$neighbors)
}else if(r$method=="rf"|r$method=="qrf"){x<- r$pred %>% filter(mtry==r$bestTune$mtry)
}else if(r$method=="svmRadial"){x<- r$pred %>% filter(sigma==r$bestTune$sigma, C==r$bestTune$C)
}else if(r$method=="lm"|r$method=="lmStepAIC"){x<- r$pred
}else if(r$method=="gbm"){x<- r$pred %>% filter(n.trees==r$bestTune$n.trees, interaction.depth==r$bestTune$interaction.depth, shrinkage==r$bestTune$shrinkage, n.minobsinnode==r$bestTune$n.minobsinnode)
}else if (r$method=="knn"){x<- r$pred %>% filter(k==r$bestTune$k)
}else {print("Not programmed for model")}
y <- x %>%
dplyr::group_by(Resample)%>%
dplyr::do(as.data.frame(oss.goof(predicted=.$pred,observed=.$obs)))
z<- as.data.frame(t(colMeans(y[c(2:7)])))
w<- as.data.frame(t(sapply(y[c(2:7)],stats::sd,na.rm=TRUE)))
z<-cbind(z,w)
rownames(z)<- r$method
colnames(z)[7:12]<- paste0("sd_",names(z[,1:6]))
rm(r,w,x,y)
as.data.frame(z)
print(z)
}
d<- oss.goofCaret(KeeneSOC)
rm(d)
d<- oss.goofCaret(KeeneSOC)
KeeneSOC
devtools::check()
load("~/R/dev/onsoilsurvey/data/keene.rda")
devtools::check()
devtools::check()
data(keene)
devtools::check()
data(KeeneSOC)
oss.goofCaret(KeeneSOC)
fit1<- oss.goofCaret(KeeneSOC)
fit1
View(fit1)
devtools::check()
devtools::check()
load("~/R/dev/onsoilsurvey/data/keene.rda")
library(devtools)
install_github("newdale/onsoilsurvey")
?onsoilsurvey
??onsoilsurvey
?oss.goof
library(onsoilsurvey)
?onsoilsurvey
??onsoilsurvey
?oss/goof
?oss.goof
library(ithir)
?goof
?oss.goof
librry(devtools)
library(devtools)
devtools::check()
library(onsoilsurvey)
library(onsoilsurvey)
?oss.edf
data(keene)
oss.edf(keene)
force(keene)
library(Rcpp)
update.packages("raster")
install.packages("raster")
install.packages("raster")
install.packages("raster")
library(onsoilsurvey)
data(keene)
library(devtools)
devtools::check()
library(onsoilsurvey)
data(keene)
oss.edf(keene)
install.packages("compositions")
?alr
library(compositions)
?alr
?clr
data(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#' data(KeeneSOC)
#' oss.getCCC(KeeneSOC)
#'
#'
getCCC <- function(Model){
x <- Model$pred
df <- x %>%
dplyr::group_by(across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
#^ this line generalizes lines look like this: group_by(k, Resample) %>%
dplyr::do(as.data.frame(get_goof(predicted=.$pred, observed=.$obs)))
#"do" is superseeded - should update this code if it's going into the package
#Converting to dataframe is somehow needed to deal with groupings I think. The standard deviation doesn't calculate properly if this line is removed
df <- data.frame(df)
df <- df %>%
dplyr::group_by(across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = sd(r2), sd_adj.r2 = sd(adj.r2), sd_concordance = sd(concordance), sd_RMSE = sd(RMSE), sd_bias = sd(bias), sd_MAE = sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
#you can't call the new columns "r2", "adj.r2" etc. because summarize will then try to take the sd of that single value later on in the same function!!!
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#' data(KeeneSOC)
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
x <- Model$pred
df <- x %>%
dplyr::group_by(across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
#^ this line generalizes lines look like this: group_by(k, Resample) %>%
dplyr::do(as.data.frame(get_goof(predicted=.$pred, observed=.$obs)))
#"do" is superseeded - should update this code if it's going into the package
#Converting to dataframe is somehow needed to deal with groupings I think. The standard deviation doesn't calculate properly if this line is removed
df <- data.frame(df)
df <- df %>%
dplyr::group_by(across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = sd(r2), sd_adj.r2 = sd(adj.r2), sd_concordance = sd(concordance), sd_RMSE = sd(RMSE), sd_bias = sd(bias), sd_MAE = sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
#you can't call the new columns "r2", "adj.r2" etc. because summarize will then try to take the sd of that single value later on in the same function!!!
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
library(dplyr)
oss.getCCC(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#' data(KeeneSOC)
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
x <- Model$pred
df <- x %>%
dplyr::group_by(across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
#^ this line generalizes lines look like this: group_by(k, Resample) %>%
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
#"do" is superseeded - should update this code if it's going into the package
#Converting to dataframe is somehow needed to deal with groupings I think. The standard deviation doesn't calculate properly if this line is removed
df <- data.frame(df)
df <- df %>%
dplyr::group_by(across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = sd(r2), sd_adj.r2 = sd(adj.r2), sd_concordance = sd(concordance), sd_RMSE = sd(RMSE), sd_bias = sd(bias), sd_MAE = sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
#you can't call the new columns "r2", "adj.r2" etc. because summarize will then try to take the sd of that single value later on in the same function!!!
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
#' @return data.frame
#' @export
#'
#' @examples
#' #Calculate fit statistics between 2 numeric vectors
#' observed<- c(3,7,4,9,6,2)
#' predicted<-c(2,5,4,7,3,5)
#' oss.goof(observed,predicted)
#'
#'
oss.goof <- function(predicted,observed){
#pull in data and create a dataframe, remove NAs, assign data to objects
data<- stats::na.omit(cbind(as.data.frame(observed),as.data.frame(predicted)))
observed <-data$observed
predicted<-data$predicted
#calculate r square and adjusted r square
rLM<- stats::lm(predicted~observed)
adj.r2 <- as.matrix(summary(rLM)$adj.r.squared)
r2 <- as.matrix(summary(rLM)$r.squared)
#calculate the root mean square error
SEP<- mean((observed-predicted)^2)
RMSE<- sqrt(SEP)
#calculate the mean absolute error
MAE<- sum(abs(observed-predicted))/nrow(data)
#calculate the bias of the estimate
bias <- (mean(mean(predicted)-observed))
#calculate Lin's Concordance correlation coefficient
ux<-mean(observed)
uy<-mean(predicted)
varxy<-mean((observed-ux) * (predicted-uy))
varx<-sum((predicted-mean(predicted))^2)/(length(predicted)-1)
vary<-sum((observed-mean(observed))^2)/(length(observed)-1)
concordance <- (2*varxy)/(varx+vary+(ux-uy)^2)
#assign the outputs to a dataframe and print it in the console for viewing
goof <-data.frame(r2=c(r2),adj.r2=c(adj.r2),concordance=c(concordance),RMSE=c(RMSE),bias=c(bias),MAE=c(MAE))
}
oss.getCCC(KeeneSOC)
library(devtools)
devtools::check()
devtools::check()
?capture.output
devtools::check()
?summarize
devtools::check()
devtools::check()
devtools::check()
devtools::document()
devtools::check()
data(KeeneSOC)
x<- KeeneSOC$pred
.<-r2<-adj.r2<-concordance<-RMSE<-bias<-MAE<-Mr2<-Madj.r2<-Mconcordance<-MRMSE<-Mbias<-MMAE<-Model<-NULL
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
View(df)
df <- data.frame(df)
View(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
View(df)
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
View(df)
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
View(df)
View(final_pars)
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
df <- data.frame(df)
View(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
names(Model$bestTune)
names(x$bestTune)
Model<- KeeneSOC
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
View(final_pars)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#'
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
.<-r2<-adj.r2<-concordance<-RMSE<-bias<-MAE<-Mr2<-Madj.r2<-Mconcordance<-MRMSE<-Mbias<-MMAE<-Model<-NULL
x <- Model$pred
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
df <- data.frame(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(utils::capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#'
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
#.<-r2<-adj.r2<-concordance<-RMSE<-bias<-MAE<-Mr2<-Madj.r2<-Mconcordance<-MRMSE<-Mbias<-MMAE<-Model<-NULL
x <- Model$pred
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
df <- data.frame(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(utils::capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#'
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
.<-r2<-adj.r2<-concordance<-RMSE<-bias<-MAE<-Mr2<-Madj.r2<-Mconcordance<-MRMSE<-Mbias<-MMAE<-NULL
x <- Model$pred
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
df <- data.frame(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(utils::capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#'
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
.<-r2<-adj.r2<-concordance<-RMSE<-bias<-MAE<-Mr2<-Madj.r2<-Mconcordance<-MRMSE<-Mbias<-MMAE<-NULL
Model<- Model
x <- Model$pred
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
df <- data.frame(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(utils::capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
devtools::check()
devtools::check()
