devtools::check()
data(keene)
devtools::check()
data(KeeneSOC)
oss.goofCaret(KeeneSOC)
fit1<- oss.goofCaret(KeeneSOC)
fit1
View(fit1)
devtools::check()
devtools::check()
load("~/R/dev/onsoilsurvey/data/keene.rda")
library(devtools)
install_github("newdale/onsoilsurvey")
?onsoilsurvey
??onsoilsurvey
?oss.goof
library(onsoilsurvey)
?onsoilsurvey
??onsoilsurvey
?oss/goof
?oss.goof
library(ithir)
?goof
?oss.goof
librry(devtools)
library(devtools)
devtools::check()
library(onsoilsurvey)
library(onsoilsurvey)
?oss.edf
data(keene)
oss.edf(keene)
force(keene)
library(Rcpp)
update.packages("raster")
install.packages("raster")
install.packages("raster")
install.packages("raster")
library(onsoilsurvey)
data(keene)
library(devtools)
devtools::check()
library(onsoilsurvey)
data(keene)
oss.edf(keene)
install.packages("compositions")
?alr
library(compositions)
?alr
?clr
data(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#' data(KeeneSOC)
#' oss.getCCC(KeeneSOC)
#'
#'
getCCC <- function(Model){
x <- Model$pred
df <- x %>%
dplyr::group_by(across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
#^ this line generalizes lines look like this: group_by(k, Resample) %>%
dplyr::do(as.data.frame(get_goof(predicted=.$pred, observed=.$obs)))
#"do" is superseeded - should update this code if it's going into the package
#Converting to dataframe is somehow needed to deal with groupings I think. The standard deviation doesn't calculate properly if this line is removed
df <- data.frame(df)
df <- df %>%
dplyr::group_by(across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = sd(r2), sd_adj.r2 = sd(adj.r2), sd_concordance = sd(concordance), sd_RMSE = sd(RMSE), sd_bias = sd(bias), sd_MAE = sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
#you can't call the new columns "r2", "adj.r2" etc. because summarize will then try to take the sd of that single value later on in the same function!!!
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#' data(KeeneSOC)
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
x <- Model$pred
df <- x %>%
dplyr::group_by(across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
#^ this line generalizes lines look like this: group_by(k, Resample) %>%
dplyr::do(as.data.frame(get_goof(predicted=.$pred, observed=.$obs)))
#"do" is superseeded - should update this code if it's going into the package
#Converting to dataframe is somehow needed to deal with groupings I think. The standard deviation doesn't calculate properly if this line is removed
df <- data.frame(df)
df <- df %>%
dplyr::group_by(across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = sd(r2), sd_adj.r2 = sd(adj.r2), sd_concordance = sd(concordance), sd_RMSE = sd(RMSE), sd_bias = sd(bias), sd_MAE = sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
#you can't call the new columns "r2", "adj.r2" etc. because summarize will then try to take the sd of that single value later on in the same function!!!
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
library(dplyr)
oss.getCCC(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#' data(KeeneSOC)
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
x <- Model$pred
df <- x %>%
dplyr::group_by(across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
#^ this line generalizes lines look like this: group_by(k, Resample) %>%
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
#"do" is superseeded - should update this code if it's going into the package
#Converting to dataframe is somehow needed to deal with groupings I think. The standard deviation doesn't calculate properly if this line is removed
df <- data.frame(df)
df <- df %>%
dplyr::group_by(across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = sd(r2), sd_adj.r2 = sd(adj.r2), sd_concordance = sd(concordance), sd_RMSE = sd(RMSE), sd_bias = sd(bias), sd_MAE = sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
#you can't call the new columns "r2", "adj.r2" etc. because summarize will then try to take the sd of that single value later on in the same function!!!
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
#' @return data.frame
#' @export
#'
#' @examples
#' #Calculate fit statistics between 2 numeric vectors
#' observed<- c(3,7,4,9,6,2)
#' predicted<-c(2,5,4,7,3,5)
#' oss.goof(observed,predicted)
#'
#'
oss.goof <- function(predicted,observed){
#pull in data and create a dataframe, remove NAs, assign data to objects
data<- stats::na.omit(cbind(as.data.frame(observed),as.data.frame(predicted)))
observed <-data$observed
predicted<-data$predicted
#calculate r square and adjusted r square
rLM<- stats::lm(predicted~observed)
adj.r2 <- as.matrix(summary(rLM)$adj.r.squared)
r2 <- as.matrix(summary(rLM)$r.squared)
#calculate the root mean square error
SEP<- mean((observed-predicted)^2)
RMSE<- sqrt(SEP)
#calculate the mean absolute error
MAE<- sum(abs(observed-predicted))/nrow(data)
#calculate the bias of the estimate
bias <- (mean(mean(predicted)-observed))
#calculate Lin's Concordance correlation coefficient
ux<-mean(observed)
uy<-mean(predicted)
varxy<-mean((observed-ux) * (predicted-uy))
varx<-sum((predicted-mean(predicted))^2)/(length(predicted)-1)
vary<-sum((observed-mean(observed))^2)/(length(observed)-1)
concordance <- (2*varxy)/(varx+vary+(ux-uy)^2)
#assign the outputs to a dataframe and print it in the console for viewing
goof <-data.frame(r2=c(r2),adj.r2=c(adj.r2),concordance=c(concordance),RMSE=c(RMSE),bias=c(bias),MAE=c(MAE))
}
oss.getCCC(KeeneSOC)
library(devtools)
devtools::check()
devtools::check()
?capture.output
devtools::check()
?summarize
devtools::check()
devtools::check()
devtools::check()
devtools::document()
devtools::check()
data(KeeneSOC)
x<- KeeneSOC$pred
.<-r2<-adj.r2<-concordance<-RMSE<-bias<-MAE<-Mr2<-Madj.r2<-Mconcordance<-MRMSE<-Mbias<-MMAE<-Model<-NULL
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
View(df)
df <- data.frame(df)
View(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
View(df)
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
View(df)
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
View(df)
View(final_pars)
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
df <- data.frame(df)
View(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
names(Model$bestTune)
names(x$bestTune)
Model<- KeeneSOC
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
View(final_pars)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#'
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
.<-r2<-adj.r2<-concordance<-RMSE<-bias<-MAE<-Mr2<-Madj.r2<-Mconcordance<-MRMSE<-Mbias<-MMAE<-Model<-NULL
x <- Model$pred
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
df <- data.frame(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(utils::capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#'
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
#.<-r2<-adj.r2<-concordance<-RMSE<-bias<-MAE<-Mr2<-Madj.r2<-Mconcordance<-MRMSE<-Mbias<-MMAE<-Model<-NULL
x <- Model$pred
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
df <- data.frame(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(utils::capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#'
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
.<-r2<-adj.r2<-concordance<-RMSE<-bias<-MAE<-Mr2<-Madj.r2<-Mconcordance<-MRMSE<-Mbias<-MMAE<-NULL
x <- Model$pred
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
df <- data.frame(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(utils::capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
#' @export
#'
#' @importFrom dplyr "%>%"
#'
#' @examples
#' #Calculate fit statistics for the best tune model
#'
#' oss.getCCC(KeeneSOC)
#'
#'
oss.getCCC <- function(Model){
.<-r2<-adj.r2<-concordance<-RMSE<-bias<-MAE<-Mr2<-Madj.r2<-Mconcordance<-MRMSE<-Mbias<-MMAE<-NULL
Model<- Model
x <- Model$pred
df <- x %>%
dplyr::group_by(dplyr::across(which(colnames(x) %in% c(names(Model$bestTune), "Resample")))) %>% #this works generally for all MLM no matter the hyperparameter names
dplyr::do(as.data.frame(oss.goof(predicted=.$pred, observed=.$obs)))
df <- data.frame(df)
df <- df %>%
dplyr::group_by(dplyr::across(which(colnames(df) %in% c(names(Model$bestTune))))) %>%
dplyr::summarize(Mr2 = mean(r2), Madj.r2 = mean(adj.r2), Mconcordance = mean(concordance), MRMSE = mean(RMSE), Mbias = mean(bias), MMAE = mean(MAE),
sd_r2 = stats::sd(r2), sd_adj.r2 = stats::sd(adj.r2), sd_concordance = stats::sd(concordance), sd_RMSE = stats::sd(RMSE), sd_bias = stats::sd(bias), sd_MAE = stats::sd(MAE),
.groups = 'drop') #the .groups = 'drop' just supresses a warning we don't need to worry about regarding how the tibble is organized
df <- data.frame(dplyr::rename(df, r2 = Mr2, adj.r2 = Madj.r2, concordance = Mconcordance, RMSE = MRMSE, bias = Mbias, MAE = MMAE))
#Save the variation in concordance across hyperparameters for the bayesian approach
df_var <- df
#Clip to the best hyperparamters and save as a separate object to include in subsequent train function
df <- df[df$concordance == max(df$concordance),]
final_pars <- df[which(colnames(df) %in% names(Model$bestTune))]
#Put optimal hyperparameters in a final column as descriptive stats of this best model
df <- cbind(method = Model$method, df)
df$final_pars <- paste(utils::capture.output(t(final_pars))[-1], collapse = " ; ")
#remove individual hyperparamter columns from dataframe
df[,which(colnames(df) %in% names(Model$bestTune))] = NULL
return(list(df, final_pars, df_var))
} ## this function filters caret training outputs and returns GOOF for best model only
oss.getCCC(KeeneSOC)
devtools::check()
devtools::check()
library(devtools)
devtools::check()
prob1 <- c(0.002, 0.020, 0.127, 0.343, 0.362, 0.119, 0.025, 0.002)
prob2<- c(0.001, 0.019, 0.325, 0.145, 0.326, 0.028, 0.153, 0.003)
sum(prob1)
sum(prob2)
barplot(prob1)
barplot(prob2)
kl_divergence(p=prob1, q=prob1, type='prob', unit='log2')
kl_divergence(p=prob1, q=prob2, type='prob', unit='log2')
kl_divergence(p=prob2, q=prob1, type='prob', unit='log2')
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
prob1 <- c(0.002, 0.020, 0.127, 0.343, 0.362, 0.119, 0.025, 0.002)
prob2<- c(0.001, 0.019, 0.325, 0.145, 0.326, 0.028, 0.153, 0.003)
sum(prob1)
barplot(prob1)
barplot(prob2)
oss.jsd(p=prob1, q=prob1, type='prob', unit='log2')
oss.jsd(p=prob1, q=prob2, type='prob', unit='log2')
oss.jsd(p=prob2, q=prob1, type='prob', unit='log2')
## KL Divergence for Continuous data - Normal Distribution
kl_cont<- function(p,q){
d1<- as.vector(p)
d2<- as.vector(q)
sigma.1<- var(d1, na.rm=TRUE)
sigma.2<- var(d2, na.rm=TRUE)
mu.1<- mean(d1, na.rm=TRUE)
mu.2<- mean(d2, na.rm=TRUE)
k<- log(sigma.2/sigma.1) + (sigma.1^2 + (mu.1 - mu.2)^2)/(2* sigma.2^2) - 0.5
return(k)
}
# we can try with theoretical data
ref<- rnorm(2000, mean=100, sd=1)
dat1<- ref + 1
dat2<- ref + 2
dat3<- ref + 4
dat4<- ref + 8
dat5<- ref + 16
d<- density(ref)
e<- density(dat1)
f<- density(dat2)
g<- density(dat3)
h<- density(dat4)
i<- density(dat5)
plot(d, xlim=c(90,125))
lines(e, col='red')
lines(f, col='blue')
lines(g, col='blue')
lines(h, col='blue')
lines(i, col='blue')
t<- rbind(kl_cont(ref, dat1),
kl_cont(ref, dat2),
kl_cont(ref, dat3),
kl_cont(ref, dat4),
kl_cont(ref, dat5))
plot(c(1,2,4,8,16),t)
devtools::check()
#' barplot(prob2)
#' # we now run the JS Divergence
#' # it is a symmetrical test, which means Q|P == P|Q
#' # if we enter the same distribution as both P and Q, we confirm a score of 0 or no divergence
#' oss.jsdist(p=prob1, q=prob1, type='prob', unit='log2')
#' # P|Q
#' oss.jsdist(p=prob1, q=prob2, type='prob', unit='log2')
#' # Q|P
#' oss.jsdist(p=prob2, q=prob1, type='prob', unit='log2')
#'
oss.jsdist<- function(p, q, type=NULL, unit='log2'){
# check to ensure user has specified data type as 'prob' or 'count' and stop function if not set
if(is.null(type)) {stop('The function argument type must be either prob or count')
}
else if (type=='prob'){
p<- p
q<- q
}
else {
p<- p/sum(p)
q<- q/sum(q)
}
m <- 0.5 * (p + q)
js<- 0.5 * oss.kld(p, m, type, unit) + 0.5 * oss.kld(q, m, type, unit)
js<- sqrt(js)
return(js)
}
prob1 <- c(0.002, 0.020, 0.127, 0.343, 0.362, 0.119, 0.025, 0.002)
prob2<- c(0.001, 0.019, 0.325, 0.145, 0.326, 0.028, 0.153, 0.003)
sum(prob1)
sum(prob2)
barplot(prob1)
barplot(prob2)
oss.jsdist(p=prob1, q=prob1, type='prob', unit='log2')
# P|Q
oss.jsdist(p=prob1, q=prob2, type='prob', unit='log2')
# Q|P
oss.jsdist(p=prob2, q=prob1, type='prob', unit='log2')
devtools::check()
devtools::check()
